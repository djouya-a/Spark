{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load covid data from excel and save it as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_excel('COVID-19.xlsx')\n",
    "df.to_csv(\"covid.csv\",sep=\",\",index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparksession object\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('log_reg').getOrCreate()\n",
    "\n",
    "#Load the dataset\n",
    "df=spark.read.csv('covid.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df=df.where(f.col(\"geoId\").isin({\"FR\", \"IT\", \"ES\"})).select('geoId','month','cases','deaths')\n",
    "df=df.orderBy(\"geoId\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+------------------+------------------+\n",
      "|summary|geoId|            month|             cases|            deaths|\n",
      "+-------+-----+-----------------+------------------+------------------+\n",
      "|  count|  707|              707|               707|               707|\n",
      "|   mean| null| 4.41018387553041|1241.1867043847242|134.04243281471005|\n",
      "| stddev| null|2.283406913644765|1914.2710420198846| 261.6102585438426|\n",
      "|    min|   ES|                1|              -766|             -1918|\n",
      "|    max|   IT|               12|             16269|              2004|\n",
      "+-------+-----+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+------+\n",
      "|geoId|month|cases|deaths|\n",
      "+-----+-----+-----+------+\n",
      "|   ES|    8| 8148|    25|\n",
      "|   ES|    8| 5114|    24|\n",
      "|   ES|    8| 7039|    16|\n",
      "+-----+-----+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter with multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+------+\n",
      "|geoId|month|cases|deaths|\n",
      "+-----+-----+-----+------+\n",
      "|   FR|    4| 1607|   427|\n",
      "|   FR|    4| 1065|   367|\n",
      "|   FR|    4| 1195|   437|\n",
      "|   FR|    4|  461|   242|\n",
      "|   FR|    4| 1537|   369|\n",
      "+-----+-----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-----+-----+------+\n",
      "|geoId|month|cases|deaths|\n",
      "+-----+-----+-----+------+\n",
      "|   FR|    4| 2633|  1438|\n",
      "|   FR|    4| 4286|  1341|\n",
      "|   FR|    4| 3777|  1417|\n",
      "|   FR|    4| 4267|  1053|\n",
      "|   FR|    4| 5233|  2004|\n",
      "+-----+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['geoId']=='FR')&(df['month']==4)).show(5)\n",
    "df.filter(df['geoId']=='FR').filter(df['deaths'] >1000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum months and exculde december 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+----------+\n",
      "|geoId|month|sum(deaths)|sum(cases)|\n",
      "+-----+-----+-----------+----------+\n",
      "|   IT|    1|          0|         3|\n",
      "|   FR|    1|          0|         6|\n",
      "|   ES|    1|          0|         0|\n",
      "|   FR|    2|          2|        51|\n",
      "|   ES|    2|          0|        54|\n",
      "+-----+-----+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.groupBy(['geoId','month']).sum('deaths','cases').orderBy('month',ascending=True)\n",
    "# Alternative\n",
    "# df2=df.groupBy(['geoId','month']).agg({'deaths':'sum','cases':'sum'}).orderBy('month',ascending=True)\n",
    "# df.groupBy([\"geoId\",\"month\"]).sum(\"deaths\").withColumnRenamed(\"sum(deaths)\", \"deaths_total\")\\\n",
    "#     .sort((\"month\")).limit(5).show()\n",
    "df2=df2.filter(df2['month']!=12)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting categorical data to numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+----------+---------+\n",
      "|geoId|month|sum(deaths)|sum(cases)|geoId_Num|\n",
      "+-----+-----+-----------+----------+---------+\n",
      "|   FR|    1|          0|         6|      1.0|\n",
      "|   ES|    1|          0|         0|      0.0|\n",
      "|   IT|    1|          0|         3|      2.0|\n",
      "|   FR|    2|          2|        51|      1.0|\n",
      "|   ES|    2|          0|        54|      0.0|\n",
      "+-----+-----+-----------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "geoId_indexer = StringIndexer(inputCol=\"geoId\", outputCol=\"geoId_Num\").fit(df2)\n",
    "df3 = geoId_indexer.transform(df2)\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+----------+---------+-------------+\n",
      "|geoId|month|sum(deaths)|sum(cases)|geoId_Num| geoId_Vector|\n",
      "+-----+-----+-----------+----------+---------+-------------+\n",
      "|   FR|    1|          0|         6|      1.0|(2,[1],[1.0])|\n",
      "|   ES|    1|          0|         0|      0.0|(2,[0],[1.0])|\n",
      "|   IT|    1|          0|         3|      2.0|    (2,[],[])|\n",
      "|   FR|    2|          2|        51|      1.0|(2,[1],[1.0])|\n",
      "|   ES|    2|          0|        54|      0.0|(2,[0],[1.0])|\n",
      "+-----+-----+-----------+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "geoId_encoder = OneHotEncoder(inputCol=\"geoId_Num\", outputCol=\"geoId_Vector\")\n",
    "df4 = geoId_encoder.fit(df3)\n",
    "df4=df4.transform(df3)\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+----------+---------+-------------+\n",
      "|geoId|month|sum(deaths)|sum(cases)|geoId_Num| geoId_Vector|\n",
      "+-----+-----+-----------+----------+---------+-------------+\n",
      "|   FR|    1|          0|         6|      1.0|(2,[1],[1.0])|\n",
      "|   ES|    1|          0|         0|      0.0|(2,[0],[1.0])|\n",
      "|   FR|    2|          2|        51|      1.0|(2,[1],[1.0])|\n",
      "+-----+-----+-----------+----------+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=df4.filter((df4['geoId_Num']!=2))\n",
    "df5.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['month','sum(deaths)','sum(cases)'], outputCol=\"features\")\n",
    "df5 = df_assembler.transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geoId: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- sum(deaths): long (nullable = true)\n",
      " |-- sum(cases): long (nullable = true)\n",
      " |-- geoId_Num: double (nullable = false)\n",
      " |-- geoId_Vector: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|geoId_Num|\n",
      "+--------------------+---------+\n",
      "|       [1.0,0.0,6.0]|      1.0|\n",
      "|       [1.0,0.0,0.0]|      0.0|\n",
      "|      [2.0,2.0,51.0]|      1.0|\n",
      "|      [2.0,0.0,54.0]|      0.0|\n",
      "|[3.0,7340.0,10421...|      0.0|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(['features','geoId_Num']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select data for building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df=df5.select(['features','geoId_Num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "#split the data \n",
    "training_df,test_df=model_df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|geoId_Num|count|\n",
      "+---------+-----+\n",
      "|      0.0|    6|\n",
      "|      1.0|    6|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.groupBy('geoId_Num').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|geoId_Num|count|\n",
      "+---------+-----+\n",
      "|      0.0|    2|\n",
      "|      1.0|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('geoId_Num').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg=LogisticRegression(labelCol='geoId_Num').fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results=log_reg.evaluate(training_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------------------------------------------------------+\n",
      "|geoId_Num|prediction|probability                                                    |\n",
      "+---------+----------+---------------------------------------------------------------+\n",
      "|1.0      |1.0       |[0.016423423002759775,0.9835765769972403,1.923337892192834E-20]|\n",
      "|1.0      |1.0       |[0.2320380887258323,0.7679619112741677,1.9996900337363204E-19] |\n",
      "|1.0      |1.0       |[0.27731177676685415,0.7226882232331459,1.93404227556266E-21]  |\n",
      "|1.0      |1.0       |[0.4628936820935944,0.5371063179064055,5.63269649559246E-24]   |\n",
      "+---------+----------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_results.filter(train_results['geoId_Num']==1).filter(train_results['prediction']==1).select(['geoId_Num','prediction','probability']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds=train_results.filter(train_results['geoId_Num']==1).filter(train_results['prediction']==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy on training dataset \n",
    "float(correct_preds)/(training_df.filter(training_df['geoId_Num']==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=log_reg.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|geoId_Num|prediction|\n",
      "+---------+----------+\n",
      "|      1.0|       0.0|\n",
      "|      0.0|       1.0|\n",
      "|      1.0|       1.0|\n",
      "|      0.0|       1.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['geoId_Num','prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#confusion matrix\n",
    "true_postives = results[(results.geoId_Num == 1) & (results.prediction == 1)].count()\n",
    "true_negatives = results[(results.geoId_Num == 0) & (results.prediction == 0)].count()\n",
    "false_positives = results[(results.geoId_Num == 0) & (results.prediction == 1)].count()\n",
    "false_negatives = results[(results.geoId_Num == 1) & (results.prediction == 0)].count()\n",
    "print (true_postives)\n",
    "print (true_negatives)\n",
    "print (false_positives)\n",
    "print (false_negatives)\n",
    "print(true_postives+true_negatives+false_positives+false_negatives)\n",
    "print (results.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
