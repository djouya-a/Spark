{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('random_forest').getOrCreate()\n",
    "df=spark.read.csv('iris_dataset.csv',inferSchema=True,header=True)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols = input_cols, outputCol='features')\n",
    "df = vec_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|         features|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|[4.7,3.2,1.3,0.2]|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-----------------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|         features|target|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|[5.1,3.5,1.4,0.2]|   0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|[4.9,3.0,1.4,0.2]|   0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|[4.7,3.2,1.3,0.2]|   0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|[4.6,3.1,1.5,0.2]|   0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|[5.0,3.6,1.4,0.2]|   0.0|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import required libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "species_indexer = StringIndexer(inputCol=\"species\", outputCol=\"target\").fit(df)\n",
    "df = species_indexer.transform(df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select data for building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|features         |target|\n",
      "+-----------------+------+\n",
      "|[5.1,3.5,1.4,0.2]|0.0   |\n",
      "|[4.9,3.0,1.4,0.2]|0.0   |\n",
      "|[4.7,3.2,1.3,0.2]|0.0   |\n",
      "|[4.6,3.1,1.5,0.2]|0.0   |\n",
      "|[5.0,3.6,1.4,0.2]|0.0   |\n",
      "|[5.4,3.9,1.7,0.4]|0.0   |\n",
      "|[4.6,3.4,1.4,0.3]|0.0   |\n",
      "|[5.0,3.4,1.5,0.2]|0.0   |\n",
      "|[4.4,2.9,1.4,0.2]|0.0   |\n",
      "|[4.9,3.1,1.5,0.1]|0.0   |\n",
      "+-----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['features','target']).show(10,False)\n",
    "model_df=df.select(['features','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=model_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf_classifier=RandomForestClassifier(labelCol='target',numTrees=50).fit(train_df)\n",
    "rf_predictions=rf_classifier.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of RF on test data is 97%\n",
      "The precision rate on test data is 97%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "rf_accuracy=MulticlassClassificationEvaluator(labelCol='target',metricName='accuracy').evaluate(rf_predictions)\n",
    "print('The accuracy of RF on test data is {0:.0%}'.format(rf_accuracy))\n",
    "\n",
    "rf_precision=MulticlassClassificationEvaluator(labelCol='target',metricName='weightedPrecision').evaluate(rf_predictions)\n",
    "print('The precision rate on test data is {0:.0%}'.format(rf_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4, {0: 0.0967, 1: 0.0181, 2: 0.4974, 3: 0.3879})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': [{'idx': 0, 'name': 'sepal_length'},\n",
       "  {'idx': 1, 'name': 'sepal_width'},\n",
       "  {'idx': 2, 'name': 'petal_length'},\n",
       "  {'idx': 3, 'name': 'petal_width'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aufhebung/Desktop/Programming/Spark/ML'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.save(\".../RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassificationModel.load(\".../RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preditions=rf.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preditions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
